{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG for GitHub issues using Hugging Face Zephyr and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers accelerate bitsandbytes transformers sentence-transformers faiss-gpu-cu12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If running in Google Colab, you may need to run this cell to make sure you're using UTF-8 locale to install LangChain\n",
    "# import locale\n",
    "# locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "ACCESS_TOKEN = getpass(\"YOUR_GITHUB_PERSONAL_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GitHubIssuesLoader\n",
    "\n",
    "loader = GitHubIssuesLoader(\n",
    "    repo=\"huggingface/peft\",\n",
    "    access_token=ACCESS_TOKEN,\n",
    "    include_prs=False,\n",
    "    state=\"all\",\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=30)\n",
    "chunked_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the embeddings + retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5112/965698010.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  db = FAISS.from_documents(chunked_docs, HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\"))\n",
      "/home/fds/miniconda3/envs/py312_torch_hf/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "db = FAISS.from_documents(chunked_docs, HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 4}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [04:07<00:00, 30.89s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the LLM chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=400,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<|system|>\n",
    "Answer the question based on your knowledge. Use the following context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "</s>\n",
    "<|user|>\n",
    "{question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\n",
    " \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"questions\"],\n",
    "    template = prompt_template,\n",
    ")\n",
    "\n",
    "llm_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do you combine multiple adapters?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain_result = llm_chain.invoke({\"context\":\"\", \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_result = rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|system|>\n",
      "Answer the question based on your knowledge. Use the following context to help:\n",
      "\n",
      "\n",
      "\n",
      "</s>\n",
      "<|user|>\n",
      "How do you combine multiple adapters?\n",
      "</s>\n",
      "<|assistant|>\n",
      "\n",
      "  To combine multiple adapters, you need to ensure that they are compatible with each other and with the devices you want to connect. Here's how you can do it:\n",
      "\n",
      "1. Identify the types of connections required: Determine which types of connectors you need for both the source device and the destination device. For example, if you want to connect a USB-C laptop to an HDMI monitor, you'll need a USB-C to HDMI adapter.\n",
      "\n",
      "2. Choose the right adapters: Look for adapters that have the appropriate connectors at both ends. For instance, you might need a USB-C to Lightning adapter to connect your iPhone to a USB-C charger.\n",
      "\n",
      "3. Connect the adapters: Plug one adapter into another until all the necessary connections are made. Make sure that the connectors fit securely into place.\n",
      "\n",
      "4. Test the connection: Turn on your devices and check whether everything is working as expected. If you encounter any issues, try rearranging the order of the adapters or using different ones.\n",
      "\n",
      "5. Label the adapters: It can be helpful to label the adapters with their functions so that you know which one to use in future setups. This will save time and prevent confusion.\n",
      "\n",
      "Remember to always follow the manufacturer's instructions when connecting multiple adapters, and avoid overloading power sources with too many devices connected simultaneously.\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|system|>\n",
      "Answer the question based on your knowledge. Use the following context to help:\n",
      "\n",
      "[Document(metadata={'url': 'https://github.com/huggingface/peft/issues/1802', 'title': 'Issues when switching between multiple adapters LoRAs ', 'creator': 'JhonDan1999', 'created_at': '2024-05-26T19:18:13Z', 'comments': 8, 'state': 'closed', 'labels': [], 'assignee': None, 'milestone': None, 'locked': False, 'number': 1802, 'is_pull_request': False}, page_content='The documentation does not mention the need to perform a merge when switching adapters. Additionally, the methods add_adapter, set_adapter, and enable_adapters do not appear to work\\r\\n\\r\\nPlease provide clarification on how to correctly switch between adapters'), Document(metadata={'url': 'https://github.com/huggingface/peft/issues/1045', 'title': 'add_weighted_adapter() is unusable, throws error: \"Invalid type <class \\'list\\'> found in target_modules\"', 'creator': 'Vectorrent', 'created_at': '2023-10-22T21:42:32Z', 'comments': 6, 'state': 'closed', 'labels': [], 'assignee': None, 'milestone': None, 'locked': False, 'number': 1045, 'is_pull_request': False}, page_content=\"If you can provide any advice, I would greatly appreciate it. I suspect that this is either unsupported and/or not fully-implemented; or, it has something to do with the way I'm attaching adapters. I've tried a bunch of alternate configurations, but I'm not having luck.\\r\\n\\r\\nThanks in advance for any help you might provide.\"), Document(metadata={'url': 'https://github.com/huggingface/peft/issues/1497', 'title': 'Cannot load adapters from Peft.from_pretrained', 'creator': 'dame-cell', 'created_at': '2024-02-21T13:37:31Z', 'comments': 0, 'state': 'closed', 'labels': [], 'assignee': None, 'milestone': None, 'locked': False, 'number': 1497, 'is_pull_request': False}, page_content='### Expected behavior\\r\\n\\r\\nI think I would want the adapters to be downloaded and then I would be able to merge my adapters into one'), Document(metadata={'url': 'https://github.com/huggingface/peft/issues/1434', 'title': 'Running multiple adapters simultaneously, reusing the output from the base model', 'creator': 'sachin-patel-qp', 'created_at': '2024-02-05T11:56:15Z', 'comments': 4, 'state': 'closed', 'labels': [], 'assignee': None, 'milestone': None, 'locked': False, 'number': 1434, 'is_pull_request': False}, page_content=\"I'm new to the concept of adapters and had a question: Is it possible to run multiple adapters simultaneously while reusing the output from the base model?\\r\\n\\r\\nFor instance, consider a base model named Base_model and 2 adapters, Adapter_1 and Adapter_2. \\r\\nCurrently, to obtain outputs from these two different adapters, we need to load and run them separately:\\r\\n\\r\\n1. Base_model --> Adapter_1 ---> Output_1\\r\\n2. Base_model --> Adapter_2 ---> Output_2\")]\n",
      "\n",
      "</s>\n",
      "<|user|>\n",
      "How do you combine multiple adapters?\n",
      "</s>\n",
      "<|assistant|>\n",
      "\n",
      "  To combine multiple adapters using PEFT (PyTorch Extension for Transformers), you can follow these steps:\n",
      "\n",
      "  1. Load each adapter separately using `Peft` class from PEFT library. You can use `Peft.from_pretrained()` method to load pre-trained adapters or create custom adapters by passing a dictionary containing the adapter weights as arguments to the `Peft` constructor.\n",
      "\n",
      "  2. Merge the adapters using the `Peft.merge_and_unfreeze()` method. This method takes a list of `Peft` objects representing the adapters you want to merge and merges them into a single `Peft` object. It also automatically unfreezes the merged adapter so you can fine-tune it further.\n",
      "\n",
      "  3. Freeze the base model and the merged adapter using the `Peft.freeze()` method. This ensures that the weights of the base model and the merged adapter remain fixed during training.\n",
      "\n",
      "  4. Train the merged adapter on your downstream task using the `Trainer` class from the Transformers library.\n",
      "\n",
      "Here's an example code snippet demonstrating how to combine multiple adapters using PEFT:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
      "from peft import Peft, PeftConfig\n",
      "\n",
      "# Load the base model and the adapters\n",
      "base_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large\")\n",
      "adapter1 = Peft.from_pretrained(\"adapter1\")\n",
      "adapter2 = Peft.from_pretrained(\"adapter2\")\n",
      "\n",
      "# Merge the adapters and freeze the base model and the merged adapter\n",
      "merged_adapter = Peft.merge_and_unfree\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_torch_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
